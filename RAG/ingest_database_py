import json
from pathlib import Path
from langchain.schema import Document
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_chroma import Chroma
from uuid import uuid4

# import the .env file
from dotenv import load_dotenv
load_dotenv()

# configuration
base_path = Path(__file__).parent
DATA_FILE = base_path / "data_export.json"

# DATA_FILE = Path("data_export.json") 
CHROMA_PATH = r"chroma_db"

# initiate the embeddings model
embeddings_model = OpenAIEmbeddings(model="text-embedding-3-large")

# initiate the vector store
vector_store = Chroma(
    collection_name="erp_malitra_collection",
    embedding_function=embeddings_model,
    persist_directory=CHROMA_PATH,
)

# loading the JSON document
if not DATA_FILE.exists():
    raise FileNotFoundError(f"‚ùå {DATA_FILE} not found!")

with open(DATA_FILE, "r", encoding="utf-8") as f:
    data = json.load(f)

# convert JSON sections into list of Documents
raw_documents = []
for section, items in data.items():
    if not items:
        continue
    # Make each section (e.g., products, users) a Document
    content = json.dumps(items, indent=2, ensure_ascii=False)
    doc = Document(page_content=content, metadata={"section": section})
    raw_documents.append(doc)

# splitting the document
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=700,
    chunk_overlap=100,
    length_function=len,
    is_separator_regex=False,
)

# creating the chunks
chunks = text_splitter.split_documents(raw_documents)

# creating unique ID's
uuids = [str(uuid4()) for _ in range(len(chunks))]

# adding chunks to vector store
vector_store.add_documents(documents=chunks, ids=uuids)